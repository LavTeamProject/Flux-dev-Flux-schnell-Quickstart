## Быстрый старт с Flux[dev] / Flux[schnell]

![изображение](https://github.com/user-attachments/assets/6409d790-3bb4-457c-a4b4-a51a45fc91d1)

В этом примере мы будем обучать LoRA Flux.1 Krea.

### Требования к оборудованию

Flux требует много **оперативной памяти системы** в дополнение к видеопамяти GPU. Простая квантование модели при запуске требует около 50 ГБ оперативной памяти системы. Если это занимает чрезмерно много времени, вам, возможно, придется оценить возможности вашего оборудования и необходимость каких-либо изменений.

При обучении каждого компонента LoRA ранга 16 (MLP, проекции, мультимодальные блоки) это приводит к использованию:
- чуть больше 30 ГБ видеопамяти при отсутствии квантования базовой модели
- чуть больше 18 ГБ видеопамяти при квантовании до int8 + весов bf16 базовой модели/LoRA
- чуть больше 13 ГБ видеопамяти при квантовании до int4 + весов bf16 базовой модели/LoRA
- чуть больше 9 ГБ видеопамяти при квантовании до NF4 + весов bf16 базовой модели/LoRA
- чуть больше 9 ГБ видеопамяти при квантовании до int2 + весов bf16 базовой модели/LoRA

Вам понадобится:
- **абсолютный минимум** - одна **3080 10G**
- **реалистичный минимум** - одна 3090 или V100 GPU
- **в идеале** несколько 4090, A6000, L40S или лучше

К счастью, они легко доступны через таких провайдеров, как [LambdaLabs](https://lambdalabs.com), которые предоставляют самые низкие доступные тарифы и локализованные кластеры для многоузлового обучения.

**В отличие от других моделей, GPU Apple в настоящее время не работают для обучения Flux.**

### Предварительные требования

Убедитесь, что у вас установлен Python; SimpleTuner хорошо работает с версиями 3.10 до 3.12.

Вы можете проверить это, запустив:

```bash
python --version
```

Если у вас не установлен Python 3.12 на Ubuntu, вы можете попробовать следующее:

```bash
apt -y install python3.12 python3.12-venv
```

#### Зависимости образа контейнера

Для Vast, RunPod и TensorDock (среди прочих) следующее будет работать на образе CUDA 12.2-12.8:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

Если `libgl1-mesa-glx` не найден, вам может понадобиться использовать `libgl1-mesa-dri` вместо этого. Ваш пробег может варьироваться.

### Установка

Клонируйте репозиторий SimpleTuner и настройте виртуальное окружение Python:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git  

cd SimpleTuner

# если python --version показывает 3.11, вы также можете использовать команду 'python' здесь.
python3.11 -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Необходимо на некоторых системах, чтобы предотвратить принятие решения, что он знает лучше нас.
poetry config virtualenvs.create false
```

**Примечание:** В настоящее время мы устанавливаем ветку `release`; ветка `main` может содержать экспериментальные функции, которые могут иметь лучшие результаты или меньшее использование памяти.

В зависимости от вашей системы вы запустите одну из 3 команд:

```bash
# Linux с NVIDIA
poetry install

# MacOS
poetry install -C install/apple

# Linux с ROCM
poetry install -C install/rocm
```

#### Дополнительные шаги для AMD ROCm

Следующее должно быть выполнено для использования AMD MI300X:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

### Настройка среды

Чтобы запустить SimpleTuner, вам нужно будет настроить файл конфигурации, каталоги набора данных и модели, а также файл конфигурации загрузчика данных.

#### Файл конфигурации

Экспериментальный скрипт `configure.py` может позволить вам полностью пропустить этот раздел через интерактивную пошаговую конфигурацию. Он содержит некоторые функции безопасности, которые помогают избежать распространенных ошибок.

**Примечание:** Это не настраивает ваш загрузчик данных. Вам все равно придется сделать это вручную позже.

Чтобы запустить его:

```bash
python configure.py
```

> ⚠️ Для пользователей, находящихся в странах, где Hugging Face Hub недоступен, вы должны добавить `HF_ENDPOINT=https://hf-mirror.com` в ваш `~/.bashrc` или `~/.zshrc` в зависимости от того, какой `$SHELL` использует ваша система.

Если вы предпочитаете настраивать вручную:

Скопируйте `config/config.json.example` в `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

Там вам, возможно, придется изменить следующие переменные:

- `model_type` - Установите это значение на `lora`.
- `model_family` - Установите это значение на `flux`.
- `model_flavour` - по умолчанию это `krea`, но может быть установлено на `dev` для обучения оригинальному релизу FLUX.1-Dev.
  - `krea` - модель FLUX.1-Krea [dev] по умолчанию, вариант с открытыми весами Krea 1, проприетарной модели, разработанной в сотрудничестве BFL и Krea.ai
  - `dev` - вариант модели Dev, предыдущий по умолчанию
  - `schnell` - вариант модели Schnell и установите любые соответствующие параметры, включая быстрый график обучения
  - `kontext` - Обучение Kontext (см. [это руководство](/documentation/quickstart/FLUX_KONTEXT.md) для конкретных указаний)
  - `fluxbooru` - модель de-distilled (требует CFG) на основе FLUX.1-Dev под названием [FluxBooru](https://hf.co/terminusresearch/fluxbooru-v0.3), созданная исследовательской группой terminus
  - `libreflux` - модель de-distilled на основе FLUX.1-Schnell, которая требует маски внимания на входах кодировщика текста T5
- `offload_during_startup` - Установите это значение на `true`, если у вас заканчивается память во время кодирования VAE.
- `pretrained_model_name_or_path` - Установите это значение на `black-forest-labs/FLUX.1-dev`.
- `pretrained_vae_model_name_or_path` - Установите это значение на `black-forest-labs/FLUX.1-dev`.
  - Обратите внимание, что вам нужно будет войти в Huggingface и получить доступ для загрузки этой модели. Мы рассмотрим вход в Huggingface позже в этом руководстве.
- `output_dir` - Установите это значение на каталог, в котором вы хотите хранить свои контрольные точки и изображения проверки. Рекомендуется использовать полный путь здесь.
- `train_batch_size` - это значение следует оставить равным 1, особенно если у вас очень маленький набор данных.
- `validation_resolution` - Поскольку Flux - это модель 1024px, вы можете установить это значение на `1024x1024`.
  - Кроме того, Flux был дообучен на нескольких корзинах с разными соотношениями сторон, и другие разрешения могут быть указаны с помощью запятых для их разделения: `1024x1024,1280x768,2048x2048`
- `validation_guidance` - Используйте то, что вы привыкли выбирать во время вывода для Flux.
- `validation_guidance_real` - Используйте >1.0, чтобы использовать CFG для вывода flux. Замедляет проверки, но дает лучшие результаты. Лучше всего работает с пустым `VALIDATION_NEGATIVE_PROMPT`.
- `validation_num_inference_steps` - Используйте где-то около 20, чтобы сэкономить время, но при этом увидеть приличное качество. Flux не очень разнообразен, и больше шагов может просто тратить время.
- `--lora_rank=4`, если вы хотите существенно уменьшить размер обучаемой LoRA. Это может помочь с использованием видеопамяти.
- Если обучаете LoRA Schnell, вам также придется вручную указать здесь `--flux_fast_schedule=true`.

- `gradient_accumulation_steps` - Предыдущее руководство рекомендовало избегать их с обучением bf16, поскольку они ухудшали модель. Дальнейшее тестирование показало, что это не обязательно так для Flux.
  - Эта опция вызывает накопление шагов обновления за несколько шагов. Это линейно увеличит время обучения, так что значение 2 заставит ваше обучение работать вдвое медленнее и занимать вдвое больше времени.
- `optimizer` - Новичкам рекомендуется придерживаться adamw_bf16, хотя optimi-lion и optimi-stableadamw также являются хорошими вариантами.
- `mixed_precision` - Новичкам следует оставить это в `bf16`
- `gradient_checkpointing` - установите это значение на true практически в каждой ситуации на каждом устройстве
- `gradient_checkpointing_interval` - это значение может быть установлено на 2 или выше на более крупных GPU, чтобы делать контрольные точки только каждые _n_ блоков. Значение 2 будет делать контрольные точки половины блоков, а 3 - одной трети.

Пользователи с несколькими GPU могут обратиться к [этому документу](/OPTIONS.md#environment-configuration-variables) для получения информации о настройке количества используемых GPU.

#### Промпты проверки

Внутри `config/config.json` находится "основной промпт проверки", который обычно является основным instance_prompt, на котором вы обучаетесь для одного субъекта или стиля. Кроме того, может быть создан файл JSON, содержащий дополнительные промпты для выполнения во время проверок.

Пример файла конфигурации `config/user_prompt_library.json.example` содержит следующий формат:

```json
{
  "nickname": "the prompt goes here",
  "another_nickname": "another prompt goes here"
}
```

Псевдонимы - это имя файла для проверки, поэтому держите их короткими и совместимыми с вашей файловой системой.

Чтобы указать тренеру на эту библиотеку промптов, добавьте ее в TRAINER_EXTRA_ARGS, добавив новую строку в конце `config.json`:
```json
  "--user_prompt_library": "config/user_prompt_library.json",
```

Набор разнообразных промптов поможет определить, не разрушается ли модель по мере обучения. В этом примере слово `<token>` следует заменить на имя вашего субъекта (instance_prompt).

```json
{
    "anime_<token>": "a breathtaking anime-style portrait of <token>, capturing her essence with vibrant colors and expressive features",
    "chef_<token>": "a high-quality, detailed photograph of <token> as a sous-chef, immersed in the art of culinary creation",
    "just_<token>": "a lifelike and intimate portrait of <token>, showcasing her unique personality and charm",
    "cinematic_<token>": "a cinematic, visually stunning photo of <token>, emphasizing her dramatic and captivating presence",
    "elegant_<token>": "an elegant and timeless portrait of <token>, exuding grace and sophistication",
    "adventurous_<token>": "a dynamic and adventurous photo of <token>, captured in an exciting, action-filled moment",
    "mysterious_<token>": "a mysterious and enigmatic portrait of <token>, shrouded in shadows and intrigue",
    "vintage_<token>": "a vintage-style portrait of <token>, evoking the charm and nostalgia of a bygone era",
    "artistic_<token>": "an artistic and abstract representation of <token>, blending creativity with visual storytelling",
    "futuristic_<token>": "a futuristic and cutting-edge portrayal of <token>, set against a backdrop of advanced technology",
    "woman": "a beautifully crafted portrait of a woman, highlighting her natural beauty and unique features",
    "man": "a powerful and striking portrait of a man, capturing his strength and character",
    "boy": "a playful and spirited portrait of a boy, capturing youthful energy and innocence",
    "girl": "a charming and vibrant portrait of a girl, emphasizing her bright personality and joy",
    "family": "a heartwarming and cohesive family portrait, showcasing the bonds and connections between loved ones"
}
```

> ℹ️ Flux - это модель потокового сопоставления, и более короткие промпты с сильным сходством приведут к практически одинаковому изображению, производимому моделью. Обязательно используйте более длинные, описательные промпты.

#### Отслеживание оценки CLIP

Если вы хотите включить оценки для оценки производительности модели, см. [этот документ](/documentation/evaluation/CLIP_SCORES.md) для получения информации о настройке и интерпретации оценок CLIP.

# Стабильная оценка потерь

Если вы хотите использовать стабильную MSE-потери для оценки производительности модели, см. [этот документ](/documentation/evaluation/EVAL_LOSS.md) для получения информации о настройке и интерпретации оценки потерь.

#### Сдвиг временного графика Flux

Модели потокового сопоставления, такие как Flux и SD3, имеют свойство, называемое "сдвиг", которое позволяет нам сдвинуть обученную часть графика временных шагов с помощью простого десятичного значения.

##### По умолчанию
По умолчанию к flux не применяется никакой сдвиг графика, что приводит к сигмоидальной колоколообразной форме распределения выборки временных шагов. Это вряд ли является идеальным подходом для Flux, но это приводит к большему объему обучения за более короткий период времени, чем автоматический сдвиг.

##### Автоматический сдвиг
Часто рекомендуемый подход - следовать нескольким последним работам и включить зависимый от разрешения сдвиг временного графика, `--flow_schedule_auto_shift`, который использует более высокие значения сдвига для больших изображений и более низкие значения сдвига для меньших изображений. Это приводит к стабильным, но потенциально посредственным результатам обучения.

##### Ручное указание
_Благодарим General Awareness из Discord за следующие примеры_

При использовании значения `--flow_schedule_shift` 0.1 (очень низкое значение) затрагиваются только мелкие детали изображения:
![изображение](https://github.com/user-attachments/assets/991ca0ad-e25a-4b13-a3d6-b4f2de1fe982)

При использовании значения `--flow_schedule_shift` 4.0 (очень высокое значение) затрагиваются крупные композиционные особенности и, возможно, цветовое пространство модели:
![изображение](https://github.com/user-attachments/assets/857a1f8a-07ab-4b75-8e6a-eecff616a28d)


#### Обучение квантованной модели

Проверено на системах Apple и NVIDIA, Hugging Face Optimum-Quanto может использоваться для снижения точности и требований к видеопамяти, обучая Flux всего на 16 ГБ.

Для пользователей `config.json`:
```json
  "base_model_precision": "int8-quanto",
  "text_encoder_1_precision": "no_change",
  "text_encoder_2_precision": "no_change",
  "lora_rank": 16,
  "max_grad_norm": 1.0,
  "base_model_default_dtype": "bf16"
```

##### Настройки, специфичные для LoRA (не LyCORIS)

```bash
# При обучении 'mmdit' мы находим очень стабильное обучение, которое заставляет модель дольше учиться.
# При обучении 'all' мы можем легко сместить распределение модели, но она более склонна к забыванию и выигрывает от высококачественных данных.
# При обучении 'all+ffs' все слои внимания обучаются в дополнение к прямой связи, что может помочь в адаптации целевой функции модели для LoRA.
# - Сообщалось, что этот режим не имеет переносимости, и такие платформы, как ComfyUI, могут не загрузить LoRA.
# Также предлагается возможность обучать только блоки 'context', но его влияние неизвестно, и он предлагается как экспериментальный выбор.
# - Расширение этого режима, 'context+ffs', также доступно, что полезно для предварительного обучения новых токенов в LoRA перед продолжением дообучения через `--init_lora`.
# Другие варианты включают 'tiny' и 'nano', которые обучают только 1 или 2 слоя.
"--flux_lora_target": "all",

# Если вы хотите использовать инициализацию LoftQ, вы не можете использовать Quanto для квантования базовой модели.
# Это, возможно, предлагает лучшую/более быструю сходимость, но работает только на устройствах NVIDIA и требует Bits n Bytes и несовместимо с Quanto.
# Другие варианты: 'default', 'gaussian' (трудно), и непроверенные варианты: 'olora' и 'pissa'.
"--lora_init_type": "loftq",
```

#### Соображения по набору данных

> ⚠️ Качество изображений для обучения важнее для Flux, чем для большинства других моделей, поскольку он впитает артефакты ваших изображений *первым*, а затем выучит концепцию/субъект.

Крайне важно иметь достаточный набор данных для обучения вашей модели. Существуют ограничения на размер набора данных, и вам нужно будет убедиться, что ваш набор данных достаточно велик для эффективного обучения вашей модели. Обратите внимание, что минимальный размер набора данных составляет `train_batch_size * gradient_accumulation_steps`, а также больше, чем `vae_batch_size`. Набор данных будет непригоден к использованию, если он слишком мал.

> ℹ️ При достаточно малом количестве изображений вы можете увидеть сообщение **no images detected in dataset** - увеличение значения `repeats` преодолеет это ограничение.

В зависимости от вашего набора данных, вам нужно будет по-разному настроить каталог набора данных и файл конфигурации загрузчика данных. В этом примере мы будем использовать [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) в качестве набора данных.

Создайте документ `--data_backend_config` (`config/multidatabackend.json`), содержащий следующее:

```json
[
  {
    "id": "pseudo-camera-10k-flux",
    "type": "local",
    "crop": true,
    "crop_aspect": "square",
    "crop_style": "center",
    "resolution": 512,
    "minimum_image_size": 512,
    "maximum_image_size": 512,
    "target_downsample_size": 512,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/flux/pseudo-camera-10k",
    "instance_data_dir": "datasets/pseudo-camera-10k",
    "disabled": false,
    "skip_file_discovery": "",
    "caption_strategy": "filename",
    "metadata_backend": "discovery",
    "repeats": 0,
    "is_regularisation_data": true
  },
  {
    "id": "dreambooth-subject",
    "type": "local",
    "crop": false,
    "resolution": 1024,
    "minimum_image_size": 1024,
    "maximum_image_size": 1024,
    "target_downsample_size": 1024,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/flux/dreambooth-subject",
    "instance_data_dir": "datasets/dreambooth-subject",
    "caption_strategy": "instanceprompt",
    "instance_prompt": "the name of your subject goes here",
    "metadata_backend": "discovery",
    "repeats": 1000
  },
  {
    "id": "dreambooth-subject-512",
    "type": "local",
    "crop": false,
    "resolution": 512,
    "minimum_image_size": 512,
    "maximum_image_size": 512,
    "target_downsample_size": 512,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/flux/dreambooth-subject-512",
    "instance_data_dir": "datasets/dreambooth-subject",
    "caption_strategy": "instanceprompt",
    "instance_prompt": "the name of your subject goes here",
    "metadata_backend": "discovery",
    "repeats": 1000
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/flux",
    "disabled": false,
    "write_batch_size": 128
  }
]
```

> ℹ️ Одновременный запуск наборов данных 512px и 1024px поддерживается и может привести к лучшей сходимости для Flux.

Затем создайте каталог `datasets`:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
    mkdir dreambooth-subject
    # поместите свои изображения в dreambooth-subject/ сейчас
popd
```

Это загрузит около 10 тысяч образцов фотографий в ваш каталог `datasets/pseudo-camera-10k`, который будет автоматически создан для вас.

Ваши изображения Dreambooth должны быть помещены в каталог `datasets/dreambooth-subject`.

#### Вход в WandB и Huggingface Hub

Вы захотите войти в WandB и HF Hub перед началом обучения, особенно если вы используете `--push_to_hub` и `--report_to=wandb`.

Если вы собираетесь вручную помещать элементы в репозиторий Git LFS, вы также должны запустить `git config --global credential.helper store`

Запустите следующие команды:

```bash
wandb login
```

и

```bash
huggingface-cli login
```

Следуйте инструкциям, чтобы войти в обе службы.

### Выполнение процесса обучения

Из каталога SimpleTuner достаточно просто запустить:

```bash
./train.sh
```

Это начнет кэширование текстовых встраиваний и выходных данных VAE на диск.

Для получения дополнительной информации см. документы [загрузчик данных](/documentation/DATALOADER.md) и [руководство](/TUTORIAL.md).

**Примечание:** Неясно, правильно ли работает обучение на корзинах с несколькими соотношениями сторон для Flux на данный момент. Рекомендуется использовать `crop_style=random` и `crop_aspect=square`.

## Советы по выводу

### Обученные с CFG LoRA (flux_guidance_value > 1)

В ComfyUI вам нужно будет пропустить Flux через другой узел под названием AdaptiveGuider. Один из членов нашего сообщества предоставил модифицированный узел здесь:

(**внешние ссылки**) [IdiotSandwichTheThird/ComfyUI-Adaptive-Guidan...](https://github.com/IdiotSandwichTheThird/ComfyUI-Adaptive-Guidance-with-disabled-init-steps) и их пример рабочего процесса [здесь](https://github.com/IdiotSandwichTheThird/ComfyUI-Adaptive-Guidance-with-disabled-init-steps/blob/master/ExampleWorkflow.json)

### CFG-дистиллированный LoRA (flux_guidance_scale == 1)

Вывод CFG-дистиллированного LoRA так же прост, как использование более низкого guidance_scale около значения, на котором обучался.

## Примечания и советы по устранению неполадок

### Конфигурация с минимальной видеопамятью

В настоящее время минимальное использование видеопамяти (9090M) может быть достигнуто с помощью:

- ОС: Ubuntu Linux 24
- GPU: Одно устройство NVIDIA CUDA (10G, 12G)
- Системная память: Примерно 50G системной памяти
- Точность базовой модели: `nf4-bnb`
- Оптимизатор: Lion 8Bit Paged, `bnb-lion8bit-paged`
- Разрешение: 512px
  - 1024px требует >= 12G VRAM
- Размер пакета: 1, нулевые шаги накопления градиента
- DeepSpeed: отключен / не настроен
- PyTorch: 2.6 Nightly (сборка 29 сентября)
- Использование `--quantize_via=cpu` для избежания ошибки outOfMemory во время запуска на картах <=16G.
- С `--attention_mechanism=sageattention` для дальнейшего снижения VRAM на 0,1 ГБ и улучшения скорости генерации изображений проверки обучения.
- Обязательно включите `--gradient_checkpointing`, иначе все, что вы делаете, не остановит его от OOMing

**ПРИМЕЧАНИЕ**: Предварительное кэширование встраиваний VAE и выходных данных текстового кодировщика может использовать больше памяти и все равно привести к OOM. Если это так, можно включить квантование текстового кодировщика и мозаичное разбиение VAE через `--vae_enable_tiling=true`. Дополнительную память можно сэкономить при запуске с помощью `--offload_during_startup=true`.

Скорость составляла примерно 1,4 итераций в секунду на 4090.

### SageAttention

При использовании `--attention_mechanism=sageattention` вывод может быть ускорен во время проверки.

**Примечание**: Это не совместимо с _каждой_ конфигурацией модели, но стоит попробовать.

### NF4-квантованное обучение

Проще говоря, NF4 - это 4-битное представление модели, что означает, что обучение имеет серьезные проблемы со стабильностью.

В ранних тестах следующее оказалось верным:
- Оптимизатор Lion вызывает коллапс модели, но использует наименьшую видеопамять; варианты AdamW помогают удержать ее вместе; bnb-adamw8bit, adamw_bf16 - отличные варианты
  - AdEMAMix плохо показал себя, но настройки не исследовались
- `--max_grad_norm=0.01` дополнительно помогает уменьшить разрушение модели, предотвращая огромные изменения модели за слишком короткое время
- NF4, AdamW8bit и более высокий размер пакета все помогают преодолеть проблемы со стабильностью, за счет большего времени, потраченного на обучение, или использовании видеопамяти
- Повышение разрешения с 512px до 1024px замедляет обучение с, например, 1,4 секунды на шаг до 3,5 секунды на шаг (размер пакета 1, 4090)
- Все, что трудно обучить на int8 или bf16, становится сложнее в NF4
- Это менее совместимо с такими опциями, как SageAttention

NF4 не работает с torch.compile, так что какую бы скорость вы ни получили, это то, что вы получите.

Если видеопамять не является проблемой (например, 48 ГБ или больше), тогда int8 с torch.compile - ваш лучший, самый быстрый вариант.

### Маскированная потеря

Если вы обучаете субъект или стиль и хотели бы маскировать один или другой, см. раздел [обучение с маскированной потерей](/documentation/DREAMBOOTH.md#masked-loss) руководства Dreambooth.

### Обучение TREAD

> ⚠️ **Экспериментально**: TREAD - это недавно реализованная функция. Хотя она функциональна, оптимальные конфигурации все еще исследуются.

[TREAD](/documentation/TREAD.md) (бумага) означает **T**oken **R**outing for **E**fficient **A**rchitecture-agnostic **D**iffusion. Это метод, который может ускорить обучение Flux, интеллектуально направляя токены через слои трансформера. Ускорение пропорционально количеству удаляемых токенов.

#### Быстрая настройка

Добавьте это в ваш `config.json`:

```json
{
  "tread_config": {
    "routes": [
      {
        "selection_ratio": 0.5,
        "start_layer_idx": 2,
        "end_layer_idx": -2
      }
    ]
  }
}
```

Эта конфигурация будет:
- Сохранять только 50% токенов изображения во время слоев 2 до предпоследнего
- Токены текста никогда не удаляются
- Ускорение обучения ~25% с минимальным влиянием на качество

#### Ключевые моменты

- **Ограниченная поддержка архитектуры** - TREAD реализован только для моделей Flux и Wan
- **Лучше всего на высоких разрешениях** - Наибольшее ускорение на 1024x1024+ из-за сложности внимания O(n²)
- **Совместимо с маскированной потерей** - Маскированные регионы автоматически сохраняются (но это снижает ускорение)
- **Работает с квантованием** - Может быть объединено с обучением int8/int4/NF4
- **Ожидайте начальный скачок потерь** - При запуске LoRA/LoKr обучение потерь будет выше изначально, но быстро исправится

#### Советы по настройке

- **Консервативно (с фокусом на качество)**: Используйте `selection_ratio` 0.3-0.5
- **Агрессивно (с фокусом на скорость)**: Используйте `selection_ratio` 0.6-0.8
- **Избегайте ранних/поздних слоев**: Не направляйте в слои 0-1 или последний слой
- **Для обучения LoRA**: Может наблюдаться небольшое замедление - экспериментируйте с разными конфигурациями
- **Более высокое разрешение = лучшее ускорение**: Наиболее полезно при 1024px и выше

#### Известное поведение

- Чем больше токенов удаляется (выше `selection_ratio`), тем быстрее обучение, но выше начальные потери
- Обучение LoRA/LoKr показывает начальный скачок потерь, который быстро исправляется по мере адаптации сети
- Некоторые конфигурации LoRA могут обучаться немного медленнее - оптимальные конфигурации все еще исследуются
- Реализация RoPE (вращающееся позиционное встраивание) функциональна, но может быть не на 100% правильной

Для подробных опций конфигурации и устранения неполадок см. [полную документацию TREAD](/documentation/TREAD.md).

### Руководство по классификатору-свободному направлению

#### Проблема
Модель Dev поступает с дистилляцией направления из коробки, что означает, что она делает очень прямую траекторию к выходам модели учителя. Это делается через вектор направления, который подается в модель во время обучения и вывода - значение этого вектора сильно влияет на тип результирующей LoRA:

#### Решение
- Значение 1.0 (**по умолчанию**) сохранит начальную дистилляцию, сделанную для модели Dev
  - Это наиболее совместимый режим
  - Вывод такой же быстрый, как и у оригинальной модели
  - Дистилляция потокового сопоставления снижает творчество и изменчивость вывода модели, как и в оригинальной модели Flux Dev (все сохраняет ту же композицию/внешний вид)
- Более высокое значение (протестировано около 3.5-4.5) повторно введет цель CFG в модель
  - Это требует, чтобы конвейер вывода имел поддержку CFG
  - Вывод на 50% медленнее и 0% увеличение VRAM **или** примерно на 20% медленнее и 20% увеличение VRAM из-за пакетного вывода CFG
  - Однако этот стиль обучения улучшает творчество и изменчивость вывода модели, что может потребоваться для определенных задач обучения

Мы можем частично повторно ввести дистилляцию в дедистиллированную модель, продолжая настраивать вашу модель с использованием значения вектора 1.0. Она никогда полностью не восстановится, но хотя бы станет более пригодной к использованию.

#### Оговорки
- Это имеет конечное влияние **либо**:
  - Увеличение задержки вывода в 2 раза, когда мы последовательно вычисляем безусловный вывод, например, с двумя отдельными проходами вперед
  - Увеличение потребления VRAM, эквивалентное использованию `num_images_per_prompt=2` и получение двух изображений во время вывода, сопровождаемое таким же процентом замедления.
    - Это часто менее экстремальное замедление, чем последовательное вычисление, но использование VRAM может быть слишком большим для большинства потребительского оборудования для обучения.
    - Этот метод *в настоящее время* не интегрирован в SimpleTuner, но работа продолжается.
- Рабочие процессы вывода для ComfyUI или других приложений (например, AUTOMATIC1111) нужно будет изменить, чтобы также включить "истинный" CFG, что может быть в настоящее время невозможно из коробки.

### Квантование
- Минимальное 8-битное квантование требуется для карты на 16 ГБ для обучения этой модели
  - В bfloat16/float16, LoRA ранга 1 занимает чуть больше 30 ГБ использования памяти
- Квантование модели до 8 бит не вредит обучению
  - Это позволяет вам увеличить размеры пакетов и, возможно, получить лучший результат
  - Ведет себя так же, как обучение с полной точностью - fp32 не сделает вашу модель лучше, чем bf16+int8.
- **int8** имеет аппаратное ускорение и поддержку `torch.compile()` на более новом оборудовании NVIDIA (3090 или лучше)
- **nf4-bnb** снижает требования к VRAM до 9 ГБ, подходя для карты на 10 ГБ (с поддержкой bfloat16)
- При загрузке LoRA в ComfyUI позже вы **должны** использовать ту же точность базовой модели, на которой обучали ваш LoRA.
- **int4** полагается на пользовательские ядра bf16 и не будет работать, если ваша карта не поддерживает bfloat16

### Сбои
- Если вы получаете SIGKILL после выгрузки текстовых кодировщиков, это означает, что у вас недостаточно системной памяти для квантования Flux.
  - Попробуйте загрузить `--base_model_precision=bf16`, но если это не сработает, вам может просто понадобиться больше памяти.
  - Попробуйте `--quantize_via=accelerator`, чтобы использовать GPU вместо этого

### Schnell
- Если вы обучаете LyCORIS LoKr на Dev, он **обычно** очень хорошо работает на Schnell уже через 4 шага.
  - Прямое обучение Schnell действительно нуждается в немного больше времени в духовке - в настоящее время результаты выглядят не очень хорошо

> ℹ️ При объединении Schnell с Dev любым способом, лицензия Dev берет верх, и она становится некоммерческой. Это не должно действительно иметь значения для большинства пользователей, но стоит отметить.

### Скорости обучения

#### LoRA (--lora_type=standard)
- LoRA в целом имеет худшую производительность, чем LoKr, для больших наборов данных
- Сообщалось, что LoRA Flux обучается аналогично LoRA SD 1.5
- Однако, модель размером 12B эмпирически показала лучшие результаты с **более низкими скоростями обучения.**
  - LoRA при 1e-3 может полностью испортить вещь. LoRA при 1e-5 почти ничего не делает.
- Ранги размером от 64 до 128 могут быть нежелательными на модели 12B из-за общих трудностей, которые увеличиваются с размером базовой модели.
  - Попробуйте сначала меньшую сеть (ранг-1, ранг-4) и продвигайтесь вверх - они будут обучаться быстрее и могут сделать все, что вам нужно.
  - Если вы обнаружите, что чрезвычайно трудно обучить вашу концепцию в модели, вам может понадобиться более высокий ранг и больше регуляризационных данных.
- Другие модели трансформаторов диффузии, такие как PixArt и SD3, значительно выигрывают от `--max_grad_norm`, и SimpleTuner сохраняет довольно высокое значение для этого по умолчанию на Flux.
  - Более низкое значение предотвратило бы слишком раннее разрушение модели, но также может сделать очень трудным изучение новых концепций, которые сильно отклоняются от распределения данных базовой модели. Модель может застрять и никогда не улучшиться.
#### LoKr (--lora_type=lycoris)
- Более высокие скорости обучения лучше для LoKr (`1e-3` с AdamW, `2e-4` с Lion)
- Другие алгоритмы нуждаются в дополнительном исследовании.
- Установка `is_regularisation_data` на такие наборы данных может помочь сохранить / предотвратить утечку и улучшить качество результирующей модели.
  - Это ведет себя иначе, чем "сохранение предыдущих потерь", которое известно тем, что удваивает размеры обучающих партий и не улучшает результаты.
  - Реализация регуляризационных данных SimpleTuner обеспечивает эффективный способ сохранения базовой модели

### Артефакты изображений
Flux немедленно поглощает плохие артефакты изображений. Так уж получилось - может потребоваться финальный тренировочный запуск только на высококачественных данных, чтобы исправить это в конце.

Когда вы делаете эти вещи (среди прочих), некоторые квадратные артефакты сетки **могут** начать появляться в образцах:
- Перетренировка с низкокачественными данными
- Использование слишком высокой скорости обучения
- Перетренировка (в общем), сеть с низкой емкостью с слишком большим количеством изображений
- Недотренировка (тоже), сеть с высокой емкостью с слишком малым количеством изображений
- Использование странных соотношений сторон или размеров обучающих данных

### Группировка по аспектам
- Тренировка слишком долго на квадратных обрезках, вероятно, не повредит этой модели слишком сильно. Вперед, это здорово и надежно.
- С другой стороны, использование естественных групп аспектов вашего набора данных может чрезмерно смещать эти формы во время вывода.
  - Это может быть желательным качеством, так как оно сохраняет зависящие от аспекта стили, такие как кинематографические вещи, от чрезмерного проникновения в другие разрешения.
  - Однако, если вы хотите улучшить результаты одинаково во многих группах аспектов, вам, возможно, придется экспериментировать с `crop_aspect=random`, что имеет свои недостатки.
- Смешивание конфигураций набора данных путем определения каталога изображений несколько раз дало действительно хорошие результаты и хорошо обобщенную модель.

### Обучение пользовательских тонко настроенных моделей Flux

Некоторые тонко настроенные модели Flux на Hugging Face Hub (такие как Dev2Pro) не имеют полной структуры каталогов, требуя установки этих конкретных опций.

Обязательно установите эти опции `flux_guidance_value`, `validation_guidance_real` и `flux_attention_masked_training` в соответствии с тем, как это сделал создатель, если эта информация доступна.
```json
{
    "model_family": "flux",
    "pretrained_model_name_or_path": "black-forest-labs/FLUX.1-dev",
    "pretrained_transformer_model_name_or_path": "ashen0209/Flux-Dev2Pro",
    "pretrained_vae_model_name_or_path": "black-forest-labs/FLUX.1-dev",
    "pretrained_transformer_subfolder": "none",
}
```

## Благодарности

Пользователи [Terminus Research](https://huggingface.co/terminusresearch), которые работали над этим, вероятно, больше, чем на их основной работе, чтобы разобраться в этом

[Lambda Labs](https://lambdalabs.com) за щедрые выделения вычислительных мощностей, которые использовались для тестов и проверок для масштабных тренировочных запусков

Особенно [@JimmyCarter](https://huggingface.co/jimmycarter) (включая добавление TREAD) и [@kaibioinfo](https://github.com/kaibioinfo) за появление с лучшими идеями и их реализацию, предлагая pull requests и проводя исчерпывающие тесты для анализа - даже осмеливаясь использовать _свои собственные лица_ для экспериментов DreamBooth.
